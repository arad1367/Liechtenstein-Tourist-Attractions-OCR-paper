{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### TThis notebook includes all the code related to the paper titled `\"Fine-Tuning Image-to-Text Models on Liechtenstein Tourist Attractions Using Microsoft GIT and Florence-2 models: A Transfer Learning Approach with Model Tracking in Weights & Biases\"`\n",
        "\n",
        "* `Pejman Ebrahimi` & `Johannes Schneider`\n",
        "* Department of Information Systems & Computer Science, University of Liechtenstein, Liechtenstein, emails: `pejman.ebrahimi@uni.li` & `johannes.schneider@uni.li`"
      ],
      "metadata": {
        "id": "ETTQaSgOQZB9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Fine-tune GIT on a custom dataset for image captioning"
      ],
      "metadata": {
        "id": "qXZtb-2WH_9O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THDS3b2UHpaK"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q datasets\n",
        "!pip install pillow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "SPBjYZV0Iowd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1. Load dataset from Hugging Face\n",
        "* link to dataset: https://huggingface.co/datasets/arad1367/Liechtenstein_tourist_attractions"
      ],
      "metadata": {
        "id": "oq1Ca0gzIGMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "ds = load_dataset(\"arad1367/Liechtenstein_tourist_attractions\", split=\"train[:99%]\")\n",
        "ds"
      ],
      "metadata": {
        "id": "HTYOTJVyH9Vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check an example image and description from dataset\n",
        "example = ds[0]\n",
        "image = example[\"image\"]\n",
        "width, height = image.size\n",
        "print(display(image.resize((int(0.3*width), int(0.3*height)))))\n",
        "\n",
        "example[\"description\"]"
      ],
      "metadata": {
        "id": "XjK20tAMH9SQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2. Create PyTorch Dataset\n",
        "> Next, we create a standard PyTorch dataset. Each item of the dataset returns the expected inputs for the model, in this case `input_ids`, `attention_mask` and `pixel_values`.\n",
        "\n",
        "> We use `GitProcessor` to turn each (image, text) pair into the expected inputs. Basically, the text gets turned into `input_ids` and `attention_mask`, and the image gets turned into `pixel_values`."
      ],
      "metadata": {
        "id": "Cp2w8rX1Iszd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class ImageCaptioningDataset(Dataset):\n",
        "    def __init__(self, dataset, processor):\n",
        "        self.dataset = dataset\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "\n",
        "        encoding = self.processor(images=item[\"image\"], text=item[\"description\"], padding=\"max_length\", return_tensors=\"pt\")\n",
        "\n",
        "        # remove batch dimension\n",
        "        encoding = {k:v.squeeze() for k,v in encoding.items()}\n",
        "\n",
        "        return encoding"
      ],
      "metadata": {
        "id": "ZRnIJY8hH9QO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3. Processor & Make train dataset"
      ],
      "metadata": {
        "id": "pkHxRwCAJdmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor\n",
        "processor = AutoProcessor.from_pretrained(\"microsoft/git-base\")\n",
        "train_dataset = ImageCaptioningDataset(ds, processor)"
      ],
      "metadata": {
        "id": "c2-lT7i-H9N_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "item = train_dataset[0]\n",
        "for k,v in item.items():\n",
        "  print(k,v.shape)"
      ],
      "metadata": {
        "id": "Q7XYCOxSI_Gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4. Create PyTorch DataLoader\n",
        "Next, we create a corresponding `PyTorch DataLoader`, which allows us to get batches of data from the dataset.\n",
        "\n",
        "We need this as neural networks `(like GIT)` are trained on batches of data, using stochastic gradient descent `(SGD)`."
      ],
      "metadata": {
        "id": "u0_mmz_1Jr7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=6)"
      ],
      "metadata": {
        "id": "TupFk2mEI_DL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_dataloader))\n",
        "for k,v in batch.items():\n",
        "  print(k,v.shape)"
      ],
      "metadata": {
        "id": "XOJoSH5hI_BQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5. Define model\n",
        "Next, we instantiate a model. We start from the `pre-trained GIT-base model` (which was already pre-trained on 4 million image-text pairs by Microsoft).\n",
        "\n",
        "Feel free to start fine-tuning another GIT model from the hub."
      ],
      "metadata": {
        "id": "qIZ35jhgJ8sp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base\")\n",
        "# processor = AutoProcessor.from_pretrained(\"microsoft/git-base\")"
      ],
      "metadata": {
        "id": "kfPzVDJcI-_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5. Dummy forward pass\n",
        "It's always good to check the initial loss on a batch. See also the blog above."
      ],
      "metadata": {
        "id": "2PY70NHoKKK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(input_ids=batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"],\n",
        "                pixel_values=batch[\"pixel_values\"],\n",
        "                labels=batch[\"input_ids\"])\n",
        "outputs.loss # Loss before fine tuning"
      ],
      "metadata": {
        "id": "VxtpHzq1I-89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.6. Train the model & Model tracking with weights and biases\n",
        "Next, let's train the model! We use native PyTorch here.\n",
        "\n",
        "We have a small dataset, we'll let the model overfit it. If it's capable of overfitting it (i.e. achieve zero loss), then that's a great way to know that everything is working properly. See also the blog above."
      ],
      "metadata": {
        "id": "s9hBkf8qKV7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "!pip install matplotlib\n",
        "!pip install nltk\n",
        "!pip install rouge-score\n",
        "!pip install rouge\n",
        "!pip install pycocoevalcap"
      ],
      "metadata": {
        "id": "N7tPZotuI-6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login() # You need your credential code for W & B"
      ],
      "metadata": {
        "id": "jHwH_8OSI-4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.7. Fine tune microsoft Git model --> loss and BLEU"
      ],
      "metadata": {
        "id": "_UBQzRiKLGEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoProcessor, AutoModelForCausalLM\n",
        "import torch\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n",
        "\n",
        "# Initialize WandB\n",
        "wandb.init(project=\"image2text-FineTune-loss-BLEU\")  # Replace with your WandB project name and entity\n",
        "\n",
        "# Load the dataset\n",
        "ds = load_dataset(\"arad1367/Liechtenstein_tourist_attractions\", split=\"train[:99%]\")\n",
        "print(ds)\n",
        "\n",
        "# Define the dataset class\n",
        "class ImageCaptioningDataset(Dataset):\n",
        "    def __init__(self, dataset, processor):\n",
        "        self.dataset = dataset\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        encoding = self.processor(images=item[\"image\"], text=item[\"description\"], padding=\"max_length\", return_tensors=\"pt\")\n",
        "        encoding = {k: v.squeeze() for k, v in encoding.items()}\n",
        "        return encoding\n",
        "\n",
        "# Load processor and dataset\n",
        "processor = AutoProcessor.from_pretrained(\"microsoft/git-base\")\n",
        "train_dataset = ImageCaptioningDataset(ds, processor)\n",
        "\n",
        "# DataLoader\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=6)\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base\")\n",
        "\n",
        "# Set up optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch: {epoch + 1}/{num_epochs}\")\n",
        "    total_loss = 0\n",
        "    predictions = []\n",
        "    references = []\n",
        "\n",
        "    for idx, batch in enumerate(tqdm(train_dataloader)):\n",
        "        input_ids = batch.pop(\"input_ids\").to(device)\n",
        "        pixel_values = batch.pop(\"pixel_values\").to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=input_ids)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Store predictions and references for evaluation\n",
        "        pred_ids = torch.argmax(outputs.logits, dim=-1)\n",
        "        predictions.append(pred_ids.cpu().numpy())\n",
        "        references.append(input_ids.cpu().numpy())\n",
        "\n",
        "    # Calculate average loss\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    wandb.log({\"loss\": avg_loss})\n",
        "\n",
        "    # Calculate BLEU score (for evaluation)\n",
        "    # Flatten the predictions and references\n",
        "    flat_predictions = [pred.flatten() for sublist in predictions for pred in sublist]\n",
        "    flat_references = [[ref.flatten()] for sublist in references for ref in sublist]\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    bleu_score = corpus_bleu(flat_references, flat_predictions)\n",
        "    wandb.log({\"bleu_score\": bleu_score})\n",
        "\n",
        "    print(f\"Average Loss: {avg_loss:.4f}, BLEU Score: {bleu_score:.4f}\")\n",
        "\n",
        "# Finish WandB run\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "NEMfPlKII-2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.8. Fine tune microsoft Git model --> All criteria"
      ],
      "metadata": {
        "id": "urO5zlZiLR-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoProcessor, AutoModelForCausalLM\n",
        "import torch\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "from rouge import Rouge\n",
        "from pycocoevalcap.cider.cider import Cider\n",
        "import nltk\n",
        "\n",
        "# Download the necessary NLTK data for CIDEr (only needs to be done once)\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize WandB\n",
        "wandb.init(project=\"image2text-FineTune-metrics-allCriteria\")  # Replace with your WandB project name and entity\n",
        "\n",
        "# Load the dataset\n",
        "ds = load_dataset(\"arad1367/Liechtenstein_tourist_attractions\", split=\"train[:99%]\")\n",
        "print(ds)\n",
        "\n",
        "# Define the dataset class\n",
        "class ImageCaptioningDataset(Dataset):\n",
        "    def __init__(self, dataset, processor):\n",
        "        self.dataset = dataset\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        # Process images and text together\n",
        "        encoding = self.processor(images=item[\"image\"], text=item[\"description\"], padding=\"max_length\", return_tensors=\"pt\")\n",
        "        encoding = {k: v.squeeze() for k, v in encoding.items()}\n",
        "        return encoding\n",
        "\n",
        "# Load processor and dataset\n",
        "processor = AutoProcessor.from_pretrained(\"microsoft/git-base\")\n",
        "train_dataset = ImageCaptioningDataset(ds, processor)\n",
        "\n",
        "# DataLoader\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=6)\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base\")\n",
        "\n",
        "# Set up optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "# Initialize ROUGE and CIDEr\n",
        "rouge = Rouge()\n",
        "cider = Cider()\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch: {epoch + 1}/{num_epochs}\")\n",
        "    total_loss = 0\n",
        "    predictions = []\n",
        "    references = []\n",
        "\n",
        "    for idx, batch in enumerate(tqdm(train_dataloader)):\n",
        "        input_ids = batch.pop(\"input_ids\").to(device)\n",
        "        pixel_values = batch.pop(\"pixel_values\").to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=input_ids)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Store predictions and references for evaluation\n",
        "        pred_ids = torch.argmax(outputs.logits, dim=-1)\n",
        "        predictions.append(pred_ids.cpu().numpy())\n",
        "        references.append(input_ids.cpu().numpy())\n",
        "\n",
        "    # Calculate average loss\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    wandb.log({\"loss\": avg_loss})\n",
        "\n",
        "    # Flatten predictions and references for evaluation\n",
        "    flat_predictions = [pred.flatten() for sublist in predictions for pred in sublist]\n",
        "    flat_references = [ref.flatten() for sublist in references for ref in sublist]\n",
        "\n",
        "    # Decode predictions and references into strings\n",
        "    decoded_predictions = [processor.decode(pred, skip_special_tokens=True) for pred in flat_predictions]\n",
        "    decoded_references = [processor.decode(ref, skip_special_tokens=True) for ref in flat_references]\n",
        "\n",
        "    # Tokenize the references and predictions for BLEU\n",
        "    tokenized_references = [[ref.split()] for ref in decoded_references]  # Tokenized reference\n",
        "    tokenized_predictions = [pred.split() for pred in decoded_predictions]  # Tokenized prediction\n",
        "\n",
        "    # Calculate BLEU score with smoothing\n",
        "    smoothing_function = SmoothingFunction().method1\n",
        "    bleu_score = corpus_bleu(tokenized_references, tokenized_predictions, smoothing_function=smoothing_function)\n",
        "    wandb.log({\"bleu_score\": bleu_score})\n",
        "\n",
        "    # Prepare CIDEr format\n",
        "    gts = {}\n",
        "    res = {}\n",
        "    for i in range(len(decoded_references)):\n",
        "        img_id = f'img_{i}'  # You can customize this id based on your dataset\n",
        "        gts[img_id] = [decoded_references[i]]\n",
        "        res[img_id] = [decoded_predictions[i]]\n",
        "\n",
        "    # Calculate CIDEr score\n",
        "    cider_score, _ = cider.compute_score(gts, res)\n",
        "    wandb.log({\"cider_score\": cider_score})\n",
        "\n",
        "    # Calculate ROUGE score\n",
        "    rouge_scores = rouge.get_scores(decoded_predictions, decoded_references, avg=True)\n",
        "    wandb.log({\"rouge-1\": rouge_scores['rouge-1']['f'],\n",
        "                \"rouge-2\": rouge_scores['rouge-2']['f'],\n",
        "                \"rouge-l\": rouge_scores['rouge-l']['f']})\n",
        "\n",
        "    print(f\"Average Loss: {avg_loss:.4f}, BLEU Score: {bleu_score:.4f}, CIDEr Score: {cider_score:.4f}, ROUGE Scores: {rouge_scores}\")\n",
        "\n",
        "# Finish WandB run\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "QiPGssmEI-z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.9. Push model to HF"
      ],
      "metadata": {
        "id": "BfUoKXpqMx3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(\"arad1367/Microsoft-git-base-Liechtenstein-TA\") # replace your space name and model name\n",
        "processor.push_to_hub(\"arad1367/Microsoft-git-base-Liechtenstein-TA\")"
      ],
      "metadata": {
        "id": "lxR-4-VcI-xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.10. Predict fine-tune model with Custom images"
      ],
      "metadata": {
        "id": "3syWUL6mNGa8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load your custom images\n",
        "image_paths = [\n",
        "    \"/content/1.jpg\",  # Change these to your custom image paths\n",
        "    \"/content/2.jpg\",\n",
        "    \"/content/3.jpg\",\n",
        "    \"/content/4.jpg\"\n",
        "]\n",
        "\n",
        "# Function to preprocess and generate caption for a single image\n",
        "def generate_caption(image_path, model, processor, device):\n",
        "    custom_image = Image.open(image_path)\n",
        "    encoding = processor(images=custom_image, return_tensors=\"pt\")\n",
        "    pixel_values = encoding['pixel_values'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(pixel_values=pixel_values, max_length=50)\n",
        "\n",
        "    predicted_caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
        "    return custom_image, predicted_caption\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Move model to the correct device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "# Generate captions for all images\n",
        "images_captions = [generate_caption(image_path, model, processor, device) for image_path in image_paths]\n",
        "\n",
        "# Function to split the caption into multiple lines\n",
        "def split_caption(caption, max_length=20):\n",
        "    words = caption.split()\n",
        "    lines = []\n",
        "    current_line = []\n",
        "    current_length = 0\n",
        "\n",
        "    for word in words:\n",
        "        if current_length + len(word) + 1 <= max_length:\n",
        "            current_line.append(word)\n",
        "            current_length += len(word) + 1\n",
        "        else:\n",
        "            lines.append(' '.join(current_line))\n",
        "            current_line = [word]\n",
        "            current_length = len(word) + 1\n",
        "\n",
        "    if current_line:\n",
        "        lines.append(' '.join(current_line))\n",
        "\n",
        "    return lines\n",
        "\n",
        "# Display images in a 2x2 grid with captions\n",
        "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for ax, (image, caption) in zip(axes, images_captions):\n",
        "    # Resize the image to 300x300 pixels\n",
        "    image = image.resize((300, 300))\n",
        "    ax.imshow(image)\n",
        "    ax.axis('off')\n",
        "\n",
        "    # Split the caption into multiple lines\n",
        "    caption_lines = split_caption(caption)\n",
        "\n",
        "    # Add a text box below the image\n",
        "    for i, line in enumerate(caption_lines):\n",
        "        ax.text(0.5, -0.1 - i * 0.05, line, fontsize=10, ha='center', transform=ax.transAxes,\n",
        "                bbox=dict(facecolor='white', alpha=0.8))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xxS62xsRI-vW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Fine-tune Florence-2 on a custom dataset for image captioning"
      ],
      "metadata": {
        "id": "pwW4VRioNh9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets flash_attn timm einops"
      ],
      "metadata": {
        "id": "RisTotMdI-tU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. Load dataset from Hugging Face datasets"
      ],
      "metadata": {
        "id": "6etR2Xr6NpM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "# Load the dataset\n",
        "data = load_dataset(\"arad1367/Liechtenstein_tourist_attractions_VQA\") # the same of previous dataset and we just add question column\n",
        "\n",
        "# Split the 'train' dataset into 'train' and 'validation'\n",
        "# e.g., 80% for training, 20% for validation\n",
        "split_data = data['train'].train_test_split(test_size=0.2)\n",
        "\n",
        "# Reconstruct the DatasetDict to include both 'train' and 'validation'\n",
        "data = DatasetDict({\n",
        "    'train': split_data['train'],\n",
        "    'validation': split_data['test']\n",
        "})\n",
        "\n",
        "# Now you can check the new data dictionary\n",
        "data['train'], data['validation']"
      ],
      "metadata": {
        "id": "nfjsRXjuI-rC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Load model & Processor\n",
        "* We can load the model using `AutoModelForCausalLM` and the processor using `AutoProcessor` classes of transformers library. Note that we need to pass `trust_remote_code as True` since this model is not a transformers model."
      ],
      "metadata": {
        "id": "AaH2vXf7N75k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoProcessor\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-base-ft\", trust_remote_code=True, revision='refs/pr/6').to(device)\n",
        "processor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-base-ft\", trust_remote_code=True, revision='refs/pr/6')\n",
        "model"
      ],
      "metadata": {
        "id": "XVRwbi-JI-os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "b28Br3q1OLxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3. Function to run example image"
      ],
      "metadata": {
        "id": "u-e6LsbdOQ6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to run the model on an example\n",
        "def run_example(task_prompt, text_input, image):\n",
        "    prompt = task_prompt + text_input\n",
        "\n",
        "    # Ensure the image is in RGB mode\n",
        "    if image.mode != \"RGB\":\n",
        "        image = image.convert(\"RGB\")\n",
        "\n",
        "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device)\n",
        "    generated_ids = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        pixel_values=inputs[\"pixel_values\"],\n",
        "        max_new_tokens=1024,\n",
        "        num_beams=3\n",
        "    )\n",
        "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
        "    parsed_answer = processor.post_process_generation(generated_text, task=task_prompt, image_size=(image.width, image.height))\n",
        "    return parsed_answer"
      ],
      "metadata": {
        "id": "d4HlnFsUOLnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test our function\n",
        "# Identify the place in the image and provide a brief description\n",
        "for idx in range(3):\n",
        "  print(run_example(\"FLVQA\", 'Identify the place in the image and provide a brief description', data['train'][idx]['image']))\n",
        "  display(data['train'][idx]['image'].resize([350, 350]))"
      ],
      "metadata": {
        "id": "1gP6vGMcOLkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4. Create Pytorch Dataset"
      ],
      "metadata": {
        "id": "OEpXmShSOl1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to construct our dataset. Note how we are adding a new task prefix <FLVQA> before the question when constructing the prompt.\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class DocVQADataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        example = self.data[idx]\n",
        "        question = \"\" + example['question']\n",
        "        first_answer = example['description']\n",
        "        # first_answer = example['description'][0]\n",
        "        image = example['image']\n",
        "        if image.mode != \"RGB\":\n",
        "            image = image.convert(\"RGB\")\n",
        "        return question, first_answer, image"
      ],
      "metadata": {
        "id": "nAa8oXF7OLiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5. DataLoader & Batch size"
      ],
      "metadata": {
        "id": "eQdcVU04Ozhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from transformers import (AdamW, AutoProcessor, get_scheduler)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    questions, answers, images = zip(*batch)\n",
        "    inputs = processor(text=list(questions), images=list(images), return_tensors=\"pt\", padding=True).to(device)\n",
        "    return inputs, answers\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = DocVQADataset(data['train'])\n",
        "val_dataset = DocVQADataset(data['validation'])\n",
        "\n",
        "# Create DataLoader\n",
        "batch_size = 6\n",
        "num_workers = 0\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, num_workers=num_workers, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn, num_workers=num_workers)"
      ],
      "metadata": {
        "id": "KtEepSR2OLfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.6. Fine tune Florence-2 (All criteria) & Model tracking with wandb"
      ],
      "metadata": {
        "id": "6YW6YJGyP62p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import wandb\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import AdamW, get_scheduler\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "from rouge import Rouge\n",
        "from pycocoevalcap.cider.cider import Cider\n",
        "import nltk\n",
        "\n",
        "# Download NLTK data (if not already installed)\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize wandb\n",
        "wandb.init(project=\"florence2_metrics_all28Sep\")  # Change project name and entity accordingly\n",
        "\n",
        "# Initialize ROUGE and CIDEr\n",
        "rouge = Rouge()\n",
        "cider = Cider()\n",
        "\n",
        "# Training function with added metrics (BLEU, ROUGE, CIDEr)\n",
        "def train_model(train_loader, val_loader, model, processor, epochs=10, lr=1e-6):\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "    num_training_steps = epochs * len(train_loader)\n",
        "    lr_scheduler = get_scheduler(\n",
        "        name=\"linear\",\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=num_training_steps,\n",
        "    )\n",
        "\n",
        "    # Watch the model in wandb (optional, to track gradients)\n",
        "    wandb.watch(model, log=\"all\", log_freq=100)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        predictions = []\n",
        "        references = []\n",
        "\n",
        "        # Training loop\n",
        "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{epochs}\"):\n",
        "            inputs, answers = batch\n",
        "\n",
        "            input_ids = inputs[\"input_ids\"].to(device)\n",
        "            pixel_values = inputs[\"pixel_values\"].to(device)\n",
        "            labels = processor.tokenizer(text=answers, return_tensors=\"pt\", padding=True, return_token_type_ids=False).input_ids.to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            # Backpropagation\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Store predictions and references for evaluation\n",
        "            pred_ids = torch.argmax(outputs.logits, dim=-1)\n",
        "            predictions.append(pred_ids.cpu().numpy())\n",
        "            references.append(labels.cpu().numpy())\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        print(f\"Average Training Loss: {avg_train_loss}\")\n",
        "\n",
        "        # Log training loss to wandb\n",
        "        wandb.log({\"Training Loss\": avg_train_loss, \"Epoch\": epoch + 1})\n",
        "\n",
        "        # Calculate metrics (BLEU, ROUGE, CIDEr) for training data\n",
        "        flat_predictions = [pred.flatten() for sublist in predictions for pred in sublist]\n",
        "        flat_references = [ref.flatten() for sublist in references for ref in sublist]\n",
        "\n",
        "        decoded_predictions = [processor.decode(pred, skip_special_tokens=True) for pred in flat_predictions]\n",
        "        decoded_references = [processor.decode(ref, skip_special_tokens=True) for ref in flat_references]\n",
        "\n",
        "        tokenized_references = [[ref.split()] for ref in decoded_references]  # Tokenized reference\n",
        "        tokenized_predictions = [pred.split() for pred in decoded_predictions]  # Tokenized prediction\n",
        "\n",
        "        # BLEU score\n",
        "        smoothing_function = SmoothingFunction().method1\n",
        "        bleu_score = corpus_bleu(tokenized_references, tokenized_predictions, smoothing_function=smoothing_function)\n",
        "        wandb.log({\"BLEU Score (Train)\": bleu_score})\n",
        "\n",
        "        # Prepare CIDEr format\n",
        "        gts = {}\n",
        "        res = {}\n",
        "        for i in range(len(decoded_references)):\n",
        "            img_id = f'img_{i}'  # You can customize this id based on your dataset\n",
        "            gts[img_id] = [decoded_references[i]]\n",
        "            res[img_id] = [decoded_predictions[i]]\n",
        "\n",
        "        # CIDEr score\n",
        "        cider_score, _ = cider.compute_score(gts, res)\n",
        "        wandb.log({\"CIDEr Score (Train)\": cider_score})\n",
        "\n",
        "        # ROUGE score\n",
        "        rouge_scores = rouge.get_scores(decoded_predictions, decoded_references, avg=True)\n",
        "        wandb.log({\n",
        "            \"ROUGE-1 (Train)\": rouge_scores['rouge-1']['f'],\n",
        "            \"ROUGE-2 (Train)\": rouge_scores['rouge-2']['f'],\n",
        "            \"ROUGE-L (Train)\": rouge_scores['rouge-l']['f']\n",
        "        })\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_predictions = []\n",
        "        val_references = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc=f\"Validation Epoch {epoch + 1}/{epochs}\"):\n",
        "                inputs, answers = batch\n",
        "\n",
        "                input_ids = inputs[\"input_ids\"].to(device)\n",
        "                pixel_values = inputs[\"pixel_values\"].to(device)\n",
        "                labels = processor.tokenizer(text=answers, return_tensors=\"pt\", padding=True, return_token_type_ids=False).input_ids.to(device)\n",
        "\n",
        "                outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n",
        "                loss = outputs.loss\n",
        "\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # Store predictions and references for validation evaluation\n",
        "                pred_ids = torch.argmax(outputs.logits, dim=-1)\n",
        "                val_predictions.append(pred_ids.cpu().numpy())\n",
        "                val_references.append(labels.cpu().numpy())\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        print(f\"Average Validation Loss: {avg_val_loss}\")\n",
        "\n",
        "        # Log validation loss to wandb\n",
        "        wandb.log({\"Validation Loss\": avg_val_loss, \"Epoch\": epoch + 1})\n",
        "\n",
        "        # Calculate metrics (BLEU, ROUGE, CIDEr) for validation data\n",
        "        flat_val_predictions = [pred.flatten() for sublist in val_predictions for pred in sublist]\n",
        "        flat_val_references = [ref.flatten() for sublist in val_references for ref in sublist]\n",
        "\n",
        "        decoded_val_predictions = [processor.decode(pred, skip_special_tokens=True) for pred in flat_val_predictions]\n",
        "        decoded_val_references = [processor.decode(ref, skip_special_tokens=True) for ref in flat_val_references]\n",
        "\n",
        "        tokenized_val_references = [[ref.split()] for ref in decoded_val_references]  # Tokenized reference\n",
        "        tokenized_val_predictions = [pred.split() for pred in decoded_val_predictions]  # Tokenized prediction\n",
        "\n",
        "        # BLEU score for validation\n",
        "        bleu_val_score = corpus_bleu(tokenized_val_references, tokenized_val_predictions, smoothing_function=smoothing_function)\n",
        "        wandb.log({\"BLEU Score (Validation)\": bleu_val_score})\n",
        "\n",
        "        # Prepare CIDEr format for validation\n",
        "        val_gts = {}\n",
        "        val_res = {}\n",
        "        for i in range(len(decoded_val_references)):\n",
        "            img_id = f'img_{i}'  # Customize based on dataset\n",
        "            val_gts[img_id] = [decoded_val_references[i]]\n",
        "            val_res[img_id] = [decoded_val_predictions[i]]\n",
        "\n",
        "        # CIDEr score for validation\n",
        "        cider_val_score, _ = cider.compute_score(val_gts, val_res)\n",
        "        wandb.log({\"CIDEr Score (Validation)\": cider_val_score})\n",
        "\n",
        "        # ROUGE score for validation\n",
        "        rouge_val_scores = rouge.get_scores(decoded_val_predictions, decoded_val_references, avg=True)\n",
        "        wandb.log({\n",
        "            \"ROUGE-1 (Validation)\": rouge_val_scores['rouge-1']['f'],\n",
        "            \"ROUGE-2 (Validation)\": rouge_val_scores['rouge-2']['f'],\n",
        "            \"ROUGE-L (Validation)\": rouge_val_scores['rouge-l']['f']\n",
        "        })\n",
        "\n",
        "        # Save model checkpoint\n",
        "        output_dir = f\"./model_checkpoints/epoch_{epoch + 1}\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        model.save_pretrained(output_dir)\n",
        "        processor.save_pretrained(output_dir)\n",
        "\n",
        "    # Finish wandb session\n",
        "    wandb.finish()\n",
        "\n",
        "# Example for freezing the vision tower parameters\n",
        "for param in model.vision_tower.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Call train_model\n",
        "train_model(train_loader, val_loader, model, processor, epochs=10)\n"
      ],
      "metadata": {
        "id": "dwnvWsEbP6Yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.7. Push model to HF\n",
        "* `Important note`: We need modify config.json after push to Hub. Othervise, after load the model you received an error. link to correct config.json: https://huggingface.co/arad1367/Florence-2-Liechtenstein-TA-OCR-VQA-modified/blob/main/config.json"
      ],
      "metadata": {
        "id": "mGs2XyX5PAzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model.push_to_hub(\"arad1367/Florence-2-Liechtenstein-TA-OCR-VQA-modified\")\n",
        "# processor.push_to_hub(\"arad1367/Florence-2-Liechtenstein-TA-OCR-VQA-modified\")"
      ],
      "metadata": {
        "id": "SEjxuL1wOLdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.8. Predict on custom image for fine tune Florence 2"
      ],
      "metadata": {
        "id": "TXx2uGwbQOWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Custom image paths\n",
        "image_paths = [\n",
        "    \"/content/Uni.jpg\",\n",
        "    \"/content/bridge.jpg\",\n",
        "    \"/content/ski.jpg\",\n",
        "    \"/content/rrr.jpg\"\n",
        "]\n",
        "\n",
        "# Function to preprocess and generate caption for a single image with the new script\n",
        "def run_example_with_caption(image_path, task_prompt, text_input):\n",
        "    # Open the custom image from the file path\n",
        "    image = Image.open(image_path)\n",
        "\n",
        "    # Combine task prompt and text input\n",
        "    prompt = f\"{task_prompt} {text_input}\".strip()\n",
        "\n",
        "    # Ensure the image is in RGB mode\n",
        "    if image.mode != \"RGB\":\n",
        "        image = image.convert(\"RGB\")\n",
        "\n",
        "    # Preprocess inputs (text + image)\n",
        "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate output from the model (keep input_ids in Long type)\n",
        "    generated_ids = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],  # Use input IDs as they are (Long tensor)\n",
        "        pixel_values=inputs[\"pixel_values\"].float(),  # Ensure pixel values are float32\n",
        "        max_new_tokens=256,  # Adjust to allow longer outputs\n",
        "        num_beams=5,         # Increase number of beams for more diversity\n",
        "        early_stopping=True   # Stop when all beams finish\n",
        "    )\n",
        "\n",
        "    # Decode the generated output\n",
        "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
        "\n",
        "    # Clean up the generated text\n",
        "    cleaned_text = generated_text.replace(\"</s>\", \"\").replace(\"<s>\", \"\").strip()\n",
        "\n",
        "    return image, cleaned_text\n",
        "\n",
        "# Function to split the caption into multiple lines for better display\n",
        "def split_caption(caption, max_length=20):\n",
        "    words = caption.split()\n",
        "    lines = []\n",
        "    current_line = []\n",
        "    current_length = 0\n",
        "\n",
        "    for word in words:\n",
        "        if current_length + len(word) + 1 <= max_length:\n",
        "            current_line.append(word)\n",
        "            current_length += len(word) + 1\n",
        "        else:\n",
        "            lines.append(' '.join(current_line))\n",
        "            current_line = [word]\n",
        "            current_length = len(word) + 1\n",
        "\n",
        "    if current_line:\n",
        "        lines.append(' '.join(current_line))\n",
        "\n",
        "    return lines\n",
        "\n",
        "# Example usage with your custom images\n",
        "task_prompt = \"FLVQA:\"\n",
        "text_input = \"Name of the place in the image and description\"\n",
        "\n",
        "# Generate captions for all custom images\n",
        "images_captions = [run_example_with_caption(image_path, task_prompt, text_input) for image_path in image_paths]\n",
        "\n",
        "# Display images in a 2x2 grid with captions\n",
        "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for ax, (image, caption) in zip(axes, images_captions):\n",
        "    # Resize the image to 300x300 pixels\n",
        "    image = image.resize((300, 300))\n",
        "    ax.imshow(image)\n",
        "    ax.axis('off')\n",
        "\n",
        "    # Split the caption into multiple lines\n",
        "    caption_lines = split_caption(caption)\n",
        "\n",
        "    # Add a text box below the image\n",
        "    for i, line in enumerate(caption_lines):\n",
        "        ax.text(0.5, -0.1 - i * 0.05, line, fontsize=10, ha='center', transform=ax.transAxes,\n",
        "                bbox=dict(facecolor='white', alpha=0.8))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9TvovWEWPdUa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}